{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Untitled0.ipynb","provenance":[],"mount_file_id":"1z-MBLb4uoke1amK6WWDF-0xLSWFeYANz","authorship_tag":"ABX9TyOFzcvX6Cp0Fpmhnk88tzEI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3o7dJiYiqz6","executionInfo":{"status":"ok","timestamp":1621646024686,"user_tz":-180,"elapsed":44389,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"5e9e96b3-abf9-44d1-9230-1b4935f81605"},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 160706 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.26-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.26-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.26-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"se959bZhjBLh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622770520007,"user_tz":-180,"elapsed":307,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"d94a0bc7-49a1-4502-9479-f224c9911d32"},"source":["cd drive/MyDrive/models4/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/models4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lo2EwxSNmTKP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622770404470,"user_tz":-180,"elapsed":315,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"803c4331-8015-41e0-af77-55e85441e3eb"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34mRanger-Deep-Learning-Optimizer\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CkABqvwIpkZs","executionInfo":{"status":"ok","timestamp":1622770508736,"user_tz":-180,"elapsed":281,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"b07e9923-eca0-4d67-9f46-157aa004ce5f"},"source":["cd ../"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s4tbu0gEh0f_","executionInfo":{"status":"ok","timestamp":1623525463595,"user_tz":-180,"elapsed":3175,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"6f631496-ba34-4349-8fde-d99e412c9dcb"},"source":["!pip install nilearn"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nilearn in /usr/local/lib/python3.7/dist-packages (0.7.1)\n","Requirement already satisfied: nibabel>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (3.0.2)\n","Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.7/dist-packages (from nilearn) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.19.5)\n","Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.0.1)\n","Requirement already satisfied: requests>=2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (2.23.0)\n","Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.4.1)\n","Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.1.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (3.0.4)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->nilearn) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->nilearn) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.18.0->nilearn) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jcxWdOXthgQp","executionInfo":{"status":"ok","timestamp":1623525478092,"user_tz":-180,"elapsed":12212,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"12acd896-415c-4bb1-fda4-2805fa67e59c"},"source":["import os\n","import cv2\n","import glob\n","import PIL\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from skimage import data\n","from skimage.util import montage \n","import skimage.transform as skTrans\n","from skimage.transform import rotate\n","from skimage.transform import resize\n","from PIL import Image, ImageOps  \n","\n","\n","# neural imaging\n","import nilearn as nl\n","import nibabel as nib\n","import nilearn.plotting as nlplt\n","!pip install git+https://github.com/miykael/gif_your_nifti # nifti to gif \n","import gif_your_nifti.core as gif2nif\n","\n","\n","# ml libs\n","import keras\n","import keras.backend as K\n","from keras.callbacks import CSVLogger\n","import tensorflow as tf\n","from tensorflow.keras.utils import plot_model\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from tensorflow.keras.models import *\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.optimizers import *\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n","\n","# Make numpy printouts easier to read.\n","np.set_printoptions(precision=3, suppress=True)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/miykael/gif_your_nifti\n","  Cloning https://github.com/miykael/gif_your_nifti to /tmp/pip-req-build-94_sd_kb\n","  Running command git clone -q https://github.com/miykael/gif_your_nifti /tmp/pip-req-build-94_sd_kb\n","Requirement already satisfied (use --upgrade to upgrade): gif-your-nifti==0.2.0 from git+https://github.com/miykael/gif_your_nifti in /usr/local/lib/python3.7/dist-packages\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gif-your-nifti==0.2.0) (1.19.5)\n","Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from gif-your-nifti==0.2.0) (3.0.2)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from gif-your-nifti==0.2.0) (2.4.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gif-your-nifti==0.2.0) (3.2.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio->gif-your-nifti==0.2.0) (7.1.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->gif-your-nifti==0.2.0) (1.15.0)\n","Building wheels for collected packages: gif-your-nifti\n","  Building wheel for gif-your-nifti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gif-your-nifti: filename=gif_your_nifti-0.2.0-cp37-none-any.whl size=6271 sha256=0eb6d285511a3b913e4ef378871ab855c73e63a1655ae959d798b9809a048da7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-yqcx4f7o/wheels/d1/0f/ed/a9190f4666474f25b7226113ec566da9ac49db8ac92fc35f2b\n","Successfully built gif-your-nifti\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"jlaUoQhRJ4hR","executionInfo":{"status":"error","timestamp":1623525284360,"user_tz":-180,"elapsed":406,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"62fa5ef3-2537-4a92-c5f7-e9c098c7ad3a"},"source":["print(ranger.__version__)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-2325719591ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'ranger' is not defined"]}]},{"cell_type":"code","metadata":{"id":"sLjZep-Oh8Rv","executionInfo":{"status":"ok","timestamp":1623525481301,"user_tz":-180,"elapsed":4,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}}},"source":["# DEFINE seg-areas  \n","SEGMENT_CLASSES = {\n","    0 : 'NOT tumor',\n","    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE\n","    2 : 'EDEMA',\n","    3 : 'ENHANCING' # original 4 -> converted into 3 later\n","}\n","\n","# there are 155 slices per volume\n","# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \n","VOLUME_SLICES = 100 \n","VOLUME_START_AT = 22 # first slice of volume that we will include"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJ9hvP1CilK3","executionInfo":{"status":"ok","timestamp":1623525482984,"user_tz":-180,"elapsed":5,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}}},"source":["TRAIN_DATASET_PATH = '/content/drive/MyDrive/braints/data/training/'\n","VALIDATION_DATASET_PATH = '/content/drive/MyDrive/braints/data/validation'"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"rrI48i56h-97","executionInfo":{"status":"ok","timestamp":1623525483989,"user_tz":-180,"elapsed":5,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}}},"source":["def jaccard_coef(y_true,y_pred,smooth = 0.5):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n","\n","def precision(y_true,y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def recall(y_true,y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"S6xFBHMTrlRr","executionInfo":{"status":"ok","timestamp":1622770938932,"user_tz":-180,"elapsed":66202,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"24556ab3-b94f-43af-81b3-4179ca28f8c7"},"source":["!pip install tensorflow-addons==0.6.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-addons==0.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/3b/1f27b6a3897e6ba44554aa0a0447444b72b862d60f0c291d37e7cde84cd7/tensorflow_addons-0.6.0-cp37-cp37m-manylinux2010_x86_64.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 7.7MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.6.0) (1.15.0)\n","Collecting tensorflow-gpu==2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/eb/bc0784af18f612838f90419cf4805c37c20ddb957f5ffe0c42144562dcfa/tensorflow_gpu-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (380.8MB)\n","\u001b[K     |████████████████████████████████| 380.8MB 46kB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (1.1.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (1.19.5)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n","\u001b[K     |████████████████████████████████| 450kB 42.4MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (3.12.4)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (1.1.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (1.34.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (0.12.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (1.12.1)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 36.5MB/s \n","\u001b[?25hCollecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (0.36.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (3.3.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (0.8.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (0.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (57.0.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (0.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (3.3.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (1.30.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (1.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (3.1.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (1.3.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (1.24.3)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (4.0.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (4.2.2)\n","Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (1.5.2)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (3.4.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons==0.6.0) (0.4.8)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=59e957be86afc1ad5ea31b553ce13e3af2e8be1f29f8640d8ffe1787d0afbe9a\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement gast==0.4.0, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 2.0.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorflow-estimator<2.6.0,>=2.5.0rc0, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: gast, tensorflow-estimator, tensorboard, keras-applications, tensorflow-gpu, tensorflow-addons\n","  Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-addons-0.6.0 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","tensorboard","tensorflow"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"2Ry3dFIUiL4I","executionInfo":{"status":"ok","timestamp":1623525487661,"user_tz":-180,"elapsed":393,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}}},"source":["IMG_SIZE=128\n","from keras.layers import Input"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5b72V9IiOn9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623525492690,"user_tz":-180,"elapsed":1006,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"56b33874-ec0f-4bd5-d873-f42d6dcad9e8"},"source":["def build_unet(inputs, ker_init, dropout):\n","    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(inputs)\n","    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv1)\n","    \n","    pool = MaxPooling2D(pool_size=(2, 2))(conv1)\n","    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool)\n","    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n","    \n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool1)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv2)\n","    \n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv3)\n","    \n","    \n","    pool4 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool4)\n","    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv5)\n","    drop5 = Dropout(dropout)(conv5)\n","\n","    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(drop5))\n","    merge7 = concatenate([conv3,up7], axis = 3)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge7)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv7)\n","\n","    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv7))\n","    merge8 = concatenate([conv2,up8], axis = 3)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge8)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv8)\n","\n","    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv8))\n","    merge9 = concatenate([conv,up9], axis = 3)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge9)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv9)\n","    \n","    up = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv9))\n","    merge = concatenate([conv1,up], axis = 3)\n","    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge)\n","    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n","    \n","    conv10 = Conv2D(4, (1,1), activation = 'softmax')(conv)\n","    \n","    return Model(inputs = inputs, outputs = conv10)\n","\n","input_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n","\n","model = build_unet(input_layer, 'he_normal', 0.2)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy',jaccard_coef,precision,recall])"],"execution_count":22,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_115), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_115/kernel:0' shape=(3, 3, 2, 32) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_115), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_115/bias:0' shape=(32,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_116), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_116/kernel:0' shape=(3, 3, 32, 32) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_116), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_116/bias:0' shape=(32,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_117), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_117/kernel:0' shape=(3, 3, 32, 64) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_117), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_117/bias:0' shape=(64,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_118), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_118/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_118), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_118/bias:0' shape=(64,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_119), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_119/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_119), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_119/bias:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_120), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_120/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_120), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_120/bias:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_121), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_121/kernel:0' shape=(3, 3, 128, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_121), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_121/bias:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_122), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_122/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_122), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_122/bias:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_123), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_123/kernel:0' shape=(3, 3, 256, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_123), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_123/bias:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_124), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_124/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_124), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_124/bias:0' shape=(512,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_125), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_125/kernel:0' shape=(2, 2, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_125), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_125/bias:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_126), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_126/kernel:0' shape=(3, 3, 512, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_126), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_126/bias:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_127), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_127/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_127), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_127/bias:0' shape=(256,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_128), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_128/kernel:0' shape=(2, 2, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_128), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_128/bias:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_129), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_129/kernel:0' shape=(3, 3, 256, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_129), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_129/bias:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_130), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_130/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_130), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_130/bias:0' shape=(128,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_131), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_131/kernel:0' shape=(2, 2, 128, 64) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_131), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_131/bias:0' shape=(64,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_132), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_132/kernel:0' shape=(3, 3, 128, 64) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_132), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_132/bias:0' shape=(64,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_133), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_133/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_133), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_133/bias:0' shape=(64,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_134), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_134/kernel:0' shape=(2, 2, 64, 32) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_134), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_134/bias:0' shape=(32,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_135), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_135/kernel:0' shape=(3, 3, 64, 32) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_135), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_135/bias:0' shape=(32,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_136), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_136/kernel:0' shape=(3, 3, 32, 32) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_136), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_136/bias:0' shape=(32,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.convolution_137), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_137/kernel:0' shape=(1, 1, 32, 4) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n","WARNING:tensorflow:\n","The following Variables were used a Lambda layer's call (tf.nn.bias_add_137), but\n","are not present in its tracked objects:\n","  <tf.Variable 'conv2d_137/bias:0' shape=(4,) dtype=float32>\n","It is possible that this is intended behavior, but it is more likely\n","an omission. This is a strong indication that this layer should be\n","formulated as a subclassed Layer rather than a Lambda layer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t9DTkS9DiaHF","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1623525338067,"user_tz":-180,"elapsed":455,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"3b0ef668-4463-40d0-9cc4-f869ea8f7002"},"source":["plot_model(model, \n","           show_shapes = True,\n","           show_dtype=False,\n","           show_layer_names = True, \n","           rankdir = 'TB', \n","           expand_nested = False, \n","           dpi = 70)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAYAAADED76LAAAABmJLR0QA/wD/AP+gvaeTAAAAJElEQVQYlWP8////fwbc4DgTHkkGBgYGhiGhgIWBgeEsHvmbAOgaBnu/TkA3AAAAAElFTkSuQmCC\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"CZhIOYNeica9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623525498567,"user_tz":-180,"elapsed":4026,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"f513feee-2ab4-4b2f-e6f7-d7c92ef4ad09"},"source":["import pandas as pd\n","import os\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","\n","TRAIN_DIR = '/content/drive/MyDrive/braints/data/training'\n","\n","def split():  \n","    survival_info_df = pd.read_csv('/content/drive/MyDrive/braints/data/training/survival_info.csv')\n","    name_mapping_df = pd.read_csv('/content/drive/MyDrive/braints/data/training/name_mapping.csv')\n","    name_mapping_df.rename({'BraTS_2020_subject_ID': 'Brats20ID'}, axis=1, inplace=True) \n","    df = survival_info_df.merge(name_mapping_df, on=\"Brats20ID\", how=\"right\")\n","    paths = []\n","    \n","    for _, row  in df.iterrows():\n","        \n","        id_ = row['Brats20ID']\n","        phase = id_.split(\"_\")[-2]\n","        \n","        if phase == 'Training':\n","            path = os.path.join(TRAIN_DIR, id_)\n","        else:\n","            path = os.path.join(TRAIN_DIR, id_)\n","        paths.append(path)\n","        \n","    df['path'] = paths\n","    \n","    train_data = df.loc[df['Age'].notnull()].reset_index(drop=True)\n","    train_data[\"Age_rank\"] =  train_data[\"Age\"] // 10 * 10\n","    train_data = train_data.loc[train_data['Brats20ID'] != 'BraTS20_Training_355'].reset_index(drop=True, )\n","    \n","    skf = StratifiedKFold(\n","        n_splits=7, random_state=55, shuffle=True\n","    )\n","    for i, (train_index, val_index) in enumerate(\n","            skf.split(train_data, train_data[\"Age_rank\"])\n","            ):\n","            train_data.loc[val_index, \"fold\"] = i\n","    \n","    test_df = df.loc[~df['Age'].notnull()].reset_index(drop=True)\n","    train_df = train_data.loc[train_data['fold'] != 0].reset_index(drop=True)\n","    val_df = train_data.loc[train_data['fold'] == 0].reset_index(drop=True)\n","    train_data.to_csv(\"train_data.csv\", index=False)\n","    \n","    train_arr = train_df['Brats20ID'].to_numpy()\n","    val_arr = val_df['Brats20ID'].to_numpy()\n","    test_arr = test_df['Brats20ID'].to_numpy()\n","    \n","    return train_arr,val_arr,test_arr\n","\n","train_ids, val_ids,test_ids = split()"],"execution_count":23,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=7.\n","  % (min_groups, self.n_splits)), UserWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"awtrf5yxJgTq","executionInfo":{"status":"ok","timestamp":1623525500378,"user_tz":-180,"elapsed":396,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}}},"source":["from tensorflow import keras"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ed1voqdljbby","executionInfo":{"status":"ok","timestamp":1623525502090,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}}},"source":["class DataGenerator(keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 4, shuffle=True):\n","        'Initialization'\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.list_IDs = list_IDs\n","        self.n_channels = n_channels\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        Batch_ids = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(Batch_ids)\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, Batch_ids):\n","        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n","        # Initialization\n","        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n","        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n","        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n","\n","        \n","        # Generate data\n","        for c, i in enumerate(Batch_ids):\n","            case_path = os.path.join(TRAIN_DATASET_PATH, i)\n","\n","            data_path = os.path.join(case_path, f'{i}_flair.nii.gz');\n","            flair = nib.load(data_path).get_fdata()    \n","\n","            data_path = os.path.join(case_path, f'{i}_t2.nii.gz');\n","            t2 = nib.load(data_path).get_fdata()\n","\n","            data_path = os.path.join(case_path, f'{i}_t1.nii.gz');\n","            t1 = nib.load(data_path).get_fdata()\n","\n","            data_path = os.path.join(case_path, f'{i}_t1ce.nii.gz');\n","            ce = nib.load(data_path).get_fdata()\n","            \n","            data_path = os.path.join(case_path, f'{i}_seg.nii.gz');\n","            seg = nib.load(data_path).get_fdata()\n","        \n","            for j in range(VOLUME_SLICES):\n","                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(t2[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(t1[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","\n","                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n","                    \n","        # Generate masks\n","        y[y==4] = 3;\n","        mask = tf.one_hot(y, 4);\n","        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n","        return X/np.max(X), Y\n","        \n","training_generator = DataGenerator(train_ids)\n","valid_generator = DataGenerator(val_ids)\n","test_generator = DataGenerator(test_ids)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"id":"3pWveVcrje_x","executionInfo":{"status":"ok","timestamp":1623525506355,"user_tz":-180,"elapsed":567,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"9b21c763-e7f5-49a8-bc0f-750b02a07435"},"source":["def showDataLayout():\n","    plt.bar([\"Train\",\"Valid\",\"Test\"],\n","    [len(train_ids), len(val_ids), len(test_ids)], align='center',color=[ 'green','red', 'blue'])\n","    plt.legend()\n","\n","    plt.ylabel('Number of images')\n","    plt.title('Data distribution')\n","\n","    plt.show()\n","    \n","showDataLayout()"],"execution_count":26,"outputs":[{"output_type":"stream","text":["No handles with labels found to put in legend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"Chel7tJ3jhnL"},"source":["csv_logger = CSVLogger('training.log', separator=',', append=False)\n","\n","\n","callbacks = [\n","     keras.callbacks.EarlyStopping(min_delta=0,\n","                               patience=8, verbose=1, mode='auto'),\n","      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n","                              patience=2, min_lr=0.000001, verbose=1),\n","  keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',\n","                             verbose=1, save_best_only=True, save_weights_only = True),\n","        csv_logger\n","    ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iLQDHpqEpREA","executionInfo":{"status":"ok","timestamp":1622744882349,"user_tz":-180,"elapsed":5681068,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"e5011cfa-277a-4916-bab6-e67ab879b9a6"},"source":["K.clear_session()\n","\n","history =  model.fit(training_generator,\n","                     epochs=35,\n","                     steps_per_epoch=len(train_ids),\n","                     callbacks= callbacks,\n","                     validation_data = valid_generator\n","                     )  \n","model.save(\"model_son.h5\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/35\n","201/201 [==============================] - 997s 5s/step - loss: 47.1602 - accuracy: 0.9591 - jaccard_coef: 0.7520 - precision: 0.9313 - recall: 0.8411 - val_loss: 19.9851 - val_accuracy: 0.9534 - val_jaccard_coef: 0.9033 - val_precision: 0.9534 - val_recall: 0.9533\n","\n","Epoch 00001: val_loss improved from inf to 19.98507, saving model to model_.01-19.985071.m5\n","Epoch 2/35\n","201/201 [==============================] - 170s 843ms/step - loss: 1.0198 - accuracy: 0.9738 - jaccard_coef: 0.8620 - precision: 0.9771 - recall: 0.9328 - val_loss: 2.6724 - val_accuracy: 0.9659 - val_jaccard_coef: 0.7164 - val_precision: 0.9722 - val_recall: 0.8878\n","\n","Epoch 00002: val_loss improved from 19.98507 to 2.67240, saving model to model_.02-2.672401.m5\n","Epoch 3/35\n","201/201 [==============================] - 170s 845ms/step - loss: 2.8555 - accuracy: 0.9753 - jaccard_coef: 0.8335 - precision: 0.9753 - recall: 0.9450 - val_loss: 0.2481 - val_accuracy: 0.9785 - val_jaccard_coef: 0.7936 - val_precision: 0.9777 - val_recall: 0.9775\n","\n","Epoch 00003: val_loss improved from 2.67240 to 0.24806, saving model to model_.03-0.248061.m5\n","Epoch 4/35\n","201/201 [==============================] - 170s 846ms/step - loss: 0.3219 - accuracy: 0.9820 - jaccard_coef: 0.8475 - precision: 0.9818 - recall: 0.9813 - val_loss: 0.2690 - val_accuracy: 0.9706 - val_jaccard_coef: 0.7127 - val_precision: 0.9712 - val_recall: 0.9691\n","\n","Epoch 00004: val_loss did not improve from 0.24806\n","Epoch 5/35\n","201/201 [==============================] - 170s 846ms/step - loss: 0.1051 - accuracy: 0.9835 - jaccard_coef: 0.8849 - precision: 0.9833 - recall: 0.9831 - val_loss: 0.1326 - val_accuracy: 0.9829 - val_jaccard_coef: 0.8724 - val_precision: 0.9826 - val_recall: 0.9826\n","\n","Epoch 00005: val_loss improved from 0.24806 to 0.13257, saving model to model_.05-0.132574.m5\n","Epoch 6/35\n","201/201 [==============================] - 171s 848ms/step - loss: 0.0856 - accuracy: 0.9844 - jaccard_coef: 0.9133 - precision: 0.9842 - recall: 0.9842 - val_loss: 0.0906 - val_accuracy: 0.9839 - val_jaccard_coef: 0.9108 - val_precision: 0.9836 - val_recall: 0.9836\n","\n","Epoch 00006: val_loss improved from 0.13257 to 0.09055, saving model to model_.06-0.090550.m5\n","Epoch 7/35\n","201/201 [==============================] - 170s 846ms/step - loss: 0.0782 - accuracy: 0.9845 - jaccard_coef: 0.9274 - precision: 0.9844 - recall: 0.9844 - val_loss: 0.0860 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9063 - val_precision: 0.9838 - val_recall: 0.9838\n","\n","Epoch 00007: val_loss improved from 0.09055 to 0.08601, saving model to model_.07-0.086005.m5\n","Epoch 8/35\n","201/201 [==============================] - 172s 852ms/step - loss: 0.0942 - accuracy: 0.9840 - jaccard_coef: 0.9152 - precision: 0.9839 - recall: 0.9839 - val_loss: 0.1056 - val_accuracy: 0.9833 - val_jaccard_coef: 0.8936 - val_precision: 0.9833 - val_recall: 0.9832\n","\n","Epoch 00008: val_loss did not improve from 0.08601\n","Epoch 9/35\n","201/201 [==============================] - 171s 849ms/step - loss: 0.0797 - accuracy: 0.9845 - jaccard_coef: 0.9267 - precision: 0.9844 - recall: 0.9844 - val_loss: 0.0944 - val_accuracy: 0.9836 - val_jaccard_coef: 0.9084 - val_precision: 0.9835 - val_recall: 0.9835\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","\n","Epoch 00009: val_loss did not improve from 0.08601\n","Epoch 10/35\n","201/201 [==============================] - 172s 855ms/step - loss: 0.0719 - accuracy: 0.9846 - jaccard_coef: 0.9409 - precision: 0.9845 - recall: 0.9845 - val_loss: 0.0864 - val_accuracy: 0.9838 - val_jaccard_coef: 0.9268 - val_precision: 0.9837 - val_recall: 0.9837\n","\n","Epoch 00010: val_loss did not improve from 0.08601\n","Epoch 11/35\n","201/201 [==============================] - 173s 855ms/step - loss: 0.0728 - accuracy: 0.9846 - jaccard_coef: 0.9406 - precision: 0.9845 - recall: 0.9845 - val_loss: 0.0811 - val_accuracy: 0.9839 - val_jaccard_coef: 0.9272 - val_precision: 0.9838 - val_recall: 0.9838\n","\n","Epoch 00011: val_loss improved from 0.08601 to 0.08109, saving model to model_.11-0.081091.m5\n","Epoch 12/35\n","201/201 [==============================] - 173s 861ms/step - loss: 0.0704 - accuracy: 0.9846 - jaccard_coef: 0.9423 - precision: 0.9845 - recall: 0.9845 - val_loss: 0.0787 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9281 - val_precision: 0.9839 - val_recall: 0.9839\n","\n","Epoch 00012: val_loss improved from 0.08109 to 0.07870, saving model to model_.12-0.078696.m5\n","Epoch 13/35\n","201/201 [==============================] - 172s 855ms/step - loss: 0.0707 - accuracy: 0.9846 - jaccard_coef: 0.9429 - precision: 0.9845 - recall: 0.9845 - val_loss: 0.0892 - val_accuracy: 0.9837 - val_jaccard_coef: 0.9220 - val_precision: 0.9836 - val_recall: 0.9837\n","\n","Epoch 00013: val_loss did not improve from 0.07870\n","Epoch 14/35\n","201/201 [==============================] - 172s 853ms/step - loss: 0.0696 - accuracy: 0.9846 - jaccard_coef: 0.9443 - precision: 0.9845 - recall: 0.9845 - val_loss: 0.0784 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9288 - val_precision: 0.9840 - val_recall: 0.9840\n","\n","Epoch 00014: val_loss improved from 0.07870 to 0.07839, saving model to model_.14-0.078386.m5\n","Epoch 15/35\n","201/201 [==============================] - 173s 860ms/step - loss: 0.0701 - accuracy: 0.9846 - jaccard_coef: 0.9432 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0795 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9265 - val_precision: 0.9840 - val_recall: 0.9840\n","\n","Epoch 00015: val_loss did not improve from 0.07839\n","Epoch 16/35\n","201/201 [==============================] - 173s 857ms/step - loss: 0.0717 - accuracy: 0.9846 - jaccard_coef: 0.9426 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0787 - val_accuracy: 0.9841 - val_jaccard_coef: 0.9269 - val_precision: 0.9840 - val_recall: 0.9840\n","\n","Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n","\n","Epoch 00016: val_loss did not improve from 0.07839\n","Epoch 17/35\n","201/201 [==============================] - 172s 853ms/step - loss: 0.0718 - accuracy: 0.9846 - jaccard_coef: 0.9402 - precision: 0.9845 - recall: 0.9845 - val_loss: 0.0816 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9270 - val_precision: 0.9839 - val_recall: 0.9839\n","\n","Epoch 00017: val_loss did not improve from 0.07839\n","Epoch 18/35\n","201/201 [==============================] - 172s 854ms/step - loss: 0.0697 - accuracy: 0.9846 - jaccard_coef: 0.9448 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0789 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9271 - val_precision: 0.9840 - val_recall: 0.9840\n","\n","Epoch 00018: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n","\n","Epoch 00018: val_loss did not improve from 0.07839\n","Epoch 19/35\n","201/201 [==============================] - 173s 859ms/step - loss: 0.0692 - accuracy: 0.9846 - jaccard_coef: 0.9449 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0773 - val_accuracy: 0.9841 - val_jaccard_coef: 0.9295 - val_precision: 0.9840 - val_recall: 0.9840\n","\n","Epoch 00019: val_loss improved from 0.07839 to 0.07731, saving model to model_.19-0.077307.m5\n","Epoch 20/35\n","201/201 [==============================] - 174s 865ms/step - loss: 0.0685 - accuracy: 0.9846 - jaccard_coef: 0.9456 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0757 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9330 - val_precision: 0.9840 - val_recall: 0.9840\n","\n","Epoch 00020: val_loss improved from 0.07731 to 0.07570, saving model to model_.20-0.075696.m5\n","Epoch 21/35\n","201/201 [==============================] - 175s 871ms/step - loss: 0.0692 - accuracy: 0.9846 - jaccard_coef: 0.9449 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0774 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9296 - val_precision: 0.9839 - val_recall: 0.9840\n","\n","Epoch 00021: val_loss did not improve from 0.07570\n","Epoch 22/35\n","201/201 [==============================] - 176s 874ms/step - loss: 0.0689 - accuracy: 0.9846 - jaccard_coef: 0.9450 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0777 - val_accuracy: 0.9839 - val_jaccard_coef: 0.9302 - val_precision: 0.9839 - val_recall: 0.9839\n","\n","Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n","\n","Epoch 00022: val_loss did not improve from 0.07570\n","Epoch 23/35\n","201/201 [==============================] - 176s 874ms/step - loss: 0.0694 - accuracy: 0.9846 - jaccard_coef: 0.9447 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0762 - val_accuracy: 0.9841 - val_jaccard_coef: 0.9308 - val_precision: 0.9840 - val_recall: 0.9840\n","\n","Epoch 00023: val_loss did not improve from 0.07570\n","Epoch 24/35\n","201/201 [==============================] - 175s 870ms/step - loss: 0.0694 - accuracy: 0.9846 - jaccard_coef: 0.9445 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0779 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9303 - val_precision: 0.9839 - val_recall: 0.9839\n","\n","Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-06.\n","\n","Epoch 00024: val_loss did not improve from 0.07570\n","Epoch 25/35\n","201/201 [==============================] - 175s 871ms/step - loss: 0.0694 - accuracy: 0.9846 - jaccard_coef: 0.9446 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0781 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9298 - val_precision: 0.9839 - val_recall: 0.9840\n","\n","Epoch 00025: val_loss did not improve from 0.07570\n","Epoch 26/35\n","201/201 [==============================] - 175s 869ms/step - loss: 0.0691 - accuracy: 0.9846 - jaccard_coef: 0.9445 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0767 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9304 - val_precision: 0.9840 - val_recall: 0.9840\n","\n","Epoch 00026: val_loss did not improve from 0.07570\n","Epoch 27/35\n","201/201 [==============================] - 177s 877ms/step - loss: 0.0700 - accuracy: 0.9846 - jaccard_coef: 0.9424 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0781 - val_accuracy: 0.9840 - val_jaccard_coef: 0.9283 - val_precision: 0.9839 - val_recall: 0.9840\n","\n","Epoch 00027: val_loss did not improve from 0.07570\n","Epoch 28/35\n","201/201 [==============================] - 178s 881ms/step - loss: 0.0699 - accuracy: 0.9846 - jaccard_coef: 0.9445 - precision: 0.9845 - recall: 0.9846 - val_loss: 0.0781 - val_accuracy: 0.9841 - val_jaccard_coef: 0.9277 - val_precision: 0.9840 - val_recall: 0.9840\n","\n","Epoch 00028: val_loss did not improve from 0.07570\n","Epoch 00028: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wfKK15qtWlWt","executionInfo":{"status":"ok","timestamp":1623525582119,"user_tz":-180,"elapsed":5361,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}}},"source":["model = keras.models.load_model('/content/drive/MyDrive/models/model_son.h5',custom_objects={'jaccard_coef':jaccard_coef,\n","                                                                                              'precision':precision,\n","                                                                                              'recall':recall},compile=False)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"YbAct2i2EYqp","executionInfo":{"status":"ok","timestamp":1623526079306,"user_tz":-180,"elapsed":468,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"d0aac907-b3a1-4ba1-f3f1-97093e6b2f02"},"source":["history = pd.read_csv('/content/drive/MyDrive/models/training.log', sep=',', engine='python')\n","\n","hist=history\n","\n","############### ########## ####### #######\n","\n","# hist=history.history\n","\n","acc=hist['accuracy']\n","val_acc=hist['val_accuracy']\n","\n","epoch=range(len(acc))\n","\n","loss=hist['loss']\n","val_loss=hist['val_loss']\n","\n","train_jaccard=hist['jaccard_coef']\n","val_jaccard=hist['val_jaccard_coef']\n","\n","precision = hist['precision']\n","val_precision = hist['val_precision']\n","\n","recall = hist['recall']\n","val_recall = hist['val_recall']\n","\n","#f,ax=plt.subplots(1,5,figsize=(16,8))\n","\n","#ax[0].plot(epoch,acc,'b',label='Training Accuracy')\n","#ax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\n","#ax[0].legend()\n","\n","#ax[1].plot(epoch,loss,'b',label='loss')\n","#ax[1].plot(epoch,val_loss,'r',label='Validation loss')\n","#ax[1].legend()\n","\n","#ax[2].plot(epoch,train_jaccard,'b',label='sensitivity')\n","#ax[2].plot(epoch,val_jaccard,'r',label='val_sensitivity')\n","#ax[2].legend()\n","\n","#ax[3].plot(epoch,precision,'b',label='precision')\n","#ax[3].plot(epoch,val_precision,'r',label='val_precision')\n","#ax[3].legend()\n","\n","#ax[4].plot(epoch,recall,'b',label='precision')\n","#ax[4].plot(epoch,val_recall,'r',label='val_precision')\n","#ax[4].legend()\n","\n","plt.plot(recall,precision,'b')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.legend()\n","\n","plt.show()"],"execution_count":32,"outputs":[{"output_type":"stream","text":["No handles with labels found to put in legend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"GckDR46Gsbua"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4FXGqzBDlKJ"},"source":["# mri type must one of 1) flair 2) t1 3) t1ce 4) t2 ------- or even 5) seg\n","# returns volume of specified study at `path`\n","def imageLoader(path):\n","    image = nib.load(path).get_fdata()\n","    X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n","    for j in range(VOLUME_SLICES):\n","        X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(image[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","        X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","        X[j +VOLUME_SLICES*c,:,:,2] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","        X[j +VOLUME_SLICES*c,:,:,3] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","\n","        y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n","    return np.array(image)\n","\n","\n","# load nifti file at `path`\n","# and load each slice with mask from volume\n","# choose the mri type & resize to `IMG_SIZE`\n","def loadDataFromDir(path, list_of_files, mriType, n_images):\n","    scans = []\n","    masks = []\n","    for i in list_of_files[:n_images]:\n","        fullPath = glob.glob( i + '/*'+ mriType +'*')[0]\n","        currentScanVolume = imageLoader(fullPath)\n","        currentMaskVolume = imageLoader( glob.glob( i + '/*seg*')[0] ) \n","        # for each slice in 3D volume, find also it's mask\n","        for j in range(0, currentScanVolume.shape[2]):\n","            scan_img = cv2.resize(currentScanVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n","            mask_img = cv2.resize(currentMaskVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n","            scans.append(scan_img[..., np.newaxis])\n","            masks.append(mask_img[..., np.newaxis])\n","    return np.array(scans, dtype='float32'), np.array(masks, dtype='float32')\n","        \n","#brains_list_test, masks_list_test = loadDataFromDir(VALIDATION_DATASET_PATH, test_directories, \"flair\", 5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLB74e9RDsbg","colab":{"base_uri":"https://localhost:8080/","height":731},"executionInfo":{"status":"error","timestamp":1622745303396,"user_tz":-180,"elapsed":1110,"user":{"displayName":"Ali Gündoğdu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIXTp8l99XXKSoTp7w_ikr5jYiGtnMuQn3FFHwlw=s64","userId":"13964773671934902567"}},"outputId":"f6c123e1-b2a5-46a2-b58d-fabe1a927091"},"source":["def predictByPath(case_path,case):\n","    files = next(os.walk(case_path))[2]\n","    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 4))\n","  #  y = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE))\n","    \n","    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_flair.nii.gz');\n","    flair=nib.load(vol_path).get_fdata()\n","    \n","    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_t2.nii.gz');\n","    t2=nib.load(vol_path).get_fdata() \n","\n","    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_t1.nii.gz');\n","    t1=nib.load(vol_path).get_fdata()\n","\n","    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_t1ce.nii.gz');\n","    ce=nib.load(vol_path).get_fdata()\n","    \n"," #   vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_seg.nii');\n"," #   seg=nib.load(vol_path).get_fdata()  \n","\n","    \n","    for j in range(VOLUME_SLICES):\n","        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n","        X[j,:,:,1] = cv2.resize(t2[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n","        X[j,:,:,2] = cv2.resize(t1[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n","        X[j,:,:,3] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n"," #       y[j,:,:] = cv2.resize(seg[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n","        \n","  #  model.evaluate(x=X,y=y[:,:,:,0], callbacks= callbacks)\n","    return model.predict(X/np.max(X), verbose=1)\n","\n","\n","def showPredictsById(case, start_slice = 60):\n","    path = f\"/content/drive/MyDrive/braints/data/training/BraTS20_Training_{case}\"\n","    gt = nib.load(os.path.join(path, f'BraTS20_Training_{case}_seg.nii.gz')).get_fdata()\n","    origImage = nib.load(os.path.join(path, f'BraTS20_Training_{case}_flair.nii.gz')).get_fdata()\n","    p = predictByPath(path,case)\n","\n","    core = p[:,:,:,1]\n","    edema= p[:,:,:,2]\n","    enhancing = p[:,:,:,3]\n","    deneme = p[:,:,:,1:4]\n","\n","    plt.figure(figsize=(18, 50))\n","    f, axarr = plt.subplots(1,3, figsize = (18, 50)) \n","\n","    #for i in range(6): # for each image, add brain background\n","     #  axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\", interpolation='none')\n","    \n","    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n","    axarr[0].title.set_text('Original image flair')\n","    curr_gt=cv2.resize(gt[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n","    axarr[1].imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n","    axarr[1].title.set_text('Ground truth')\n","    axarr[2].imshow(p[start_slice,:,:,1:4], cmap=\"gray\", interpolation='none', alpha=0.3)\n","    axarr[2].title.set_text('predicted')\n","    plt.show()\n","    \n","    \n","showPredictsById(case=test_ids[7][-3:])\n","showPredictsById(case=test_ids[8][-3:])\n","showPredictsById(case=test_ids[9][-3:])\n","showPredictsById(case=test_ids[10][-3:])\n","showPredictsById(case=test_ids[11][-3:])\n","showPredictsById(case=test_ids[12][-3:])\n","showPredictsById(case=test_ids[13][-3:])"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-9a7563f9a1b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mshowPredictsById\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mshowPredictsById\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mshowPredictsById\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-9a7563f9a1b0>\u001b[0m in \u001b[0;36mshowPredictsById\u001b[0;34m(case, start_slice)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'BraTS20_Training_{case}_seg.nii.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0morigImage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'BraTS20_Training_{case}_flair.nii.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictByPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-9a7563f9a1b0>\u001b[0m in \u001b[0;36mpredictByPath\u001b[0;34m(case_path, case)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m#  model.evaluate(x=X,y=y[:,:,:,0], callbacks= callbacks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1725\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1559 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1552 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1525 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:270 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 128, 128, 2), found shape=(None, 128, 128, 4)\n"]}]}]}